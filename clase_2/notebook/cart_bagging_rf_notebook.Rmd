---
title: "Implementando modelos basados en árboles en `caret`"
author: "Germán Rosati"
output: html_notebook
---

## Objetivos

- Introducir los principales conceptos alrededor de la estimación de modelos basados en árboles de decisión (cart, bagging, random forest) 
- Mostrar su implementación en `caret`


## El problema

Segumos con nuestro problema central: predecir los ingresos de la ocupación principal (`p21`) en la EPH del segundo trimestre del 2015. Pero en esta oportunidad queremos evaluar si podemos predecir la no respuesta. Es decir, entrenar un modelo que nos permita predecir qué tan probable es que una persona NO responda ingresos.

Ya hemos preprocesado los datos y estamos listos 

Lo primero que tenemos que hacer es importar las librerías con las que vamos a trabajar:


```{r, message=FALSE}
library(caret)
library(tidyverse)
library(rpart)
```


Luego, cargamos los datos y formateamos un poco algunas etiquetas:


```{r}
load('../data/EPH_2015_II.RData')

data$pp03i<-factor(data$pp03i, labels=c('1-SI', '2-No', '9-NS'))



data$intensi<-factor(data$intensi, labels=c('1-Sub_dem', '2-SO_no_dem', 
                                            '3-Ocup.pleno', '4-Sobreoc',
                                            '5-No trabajo', '9-NS'))

data$pp07a<-factor(data$pp07a, labels=c('0-NC',
                                        '1-Menos de un mes',
                                        '2-1 a 3 meses',
                                        '3-3 a 6 meses',
                                        '4-6 a 12 meses',
                                        '5-12 a 60 meses',
                                        '6-Más de 60 meses',
                                        '9-NS'))



data <- data %>%
        mutate(imp_inglab1=factor(imp_inglab1, labels=c('non_miss','miss')))
```

Ahora, nuestra variable a predecir es el indicador `imp_inglab`. Por ende, elimonamos la $p21$.

```{r}

df_train <- data %>%
        select(-p21)

```

Lo primero que vamos a hacer es crear una partición de datos:

```{r}
set.seed(1234)
tr_index <- createDataPartition(y=df_train$imp_inglab1,
                                p=0.8,
                                list=FALSE)
```

Y generamos dos datasets

```{r}
train <- df_train[tr_index,]
test <- df_train[-tr_index,]
```



## Entrenando modelos (`train()`)
### CART -  Classification and Regression Trees

Empecemos por entrenar algunos árboles simples para tener una idea del proceso. Para entrenar modelos sin tunear hiperparámetros, tenemos que definir un objeto `trainControl` con `method='none'`.


```{r}
fitControl <- trainControl(method = "none", classProbs = FALSE)
```

Ahora podemos entrenar un árbol poco profundo... digamos, 3. 

```{r}
cart_tune <- train(imp_inglab1 ~ . , 
                 data = df_train, 
                 method = "rpart2", 
                 trControl = fitControl,
                 tuneGrid = data.frame(maxdepth=3),
                 control = rpart.control(minsplit = 1,
                                         minbucket = 1,
                                        cp=0.00000001)
)
```

Y podemos plotearlo de forma fea:

```{r}
plot(cart_tune$finalModel)
text(cart_tune$finalModel, pretty=1)
```

O de forma bonita:

```{r}
library(rpart.plot)
```

```{r}
rpart.plot(cart_tune$finalModel)
```


Testeemos la performance de este árbol:

```{r}
table(predict(cart_tune, df_train) , df_train$imp_inglab1)
```

¿Qué conclusión pueden sacar al respecto?

Entrenen, ahora, un segundo árbol pero más complejo: `maxdepth=10`.


```{r}
cart_tune <- train(imp_inglab1 ~ . , 
                 data = df_train, 
                 method = "rpart2", 
                 trControl = fitControl,
                 tuneGrid = data.frame(maxdepth=10),
                 control = rpart.control(cp=0.0001)
)
```

```{r}
rpart.plot(cart_tune$finalModel)
```

```{r}
table(predict(cart_tune, df_train) , df_train$imp_inglab1)
```

Hasta aquí estuvimos haciendo trampa. Vamos a ahora a tunear el parámetro de profundidad de forma correcta.


### Seteando la partición para evaluar

Primero, fijamos la semilla aleatoria (para asegurarnos la posibilidad de replicabilidad)


```{r}
set.seed(789)
```

Podemos usar la función `createFolds()` para generar los índices. Aquí, pas

```{r}
cv_index <- createFolds(y = train$imp_inglab1,
                        k=5,
                        list=TRUE,
                        returnTrain=TRUE)
```

Finalmente, especificamos el diseño de remuestreo mediante la función `trainControl`:

```{r}
fitControl <- trainControl(
        index=cv_index,
        method="cv",
        number=5
        )
```


```{r}
grid <- expand.grid(maxdepth=c(1, 2, 4, 8, 10, 15, 20))
```

Y volvemos a entrenar el modelo:

```{r warning=FALSE}
cart_tune <- train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "rpart2", 
                 trControl = fitControl,
                 tuneGrid = grid,
                 control = rpart.control(cp=0.000001)
)

cart_tune
```


## Seleccionando el mejor modelo

Una vez finalizado el proceso de tunning de los hiperparámetros, podemos proceder a elegir cuál es el mejor modelo y entrenarlo sobre todo el dataset. Podemos ver que el mejor es un árbol que parece demasiado complejo `maxdepth=15`, por ello, vamos a elegir uno un poco más interpreable

```{r}
cart_final <- train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "rpart2", 
                 tuneGrid = data.frame(maxdepth=6),
                 control = rpart.control(cp=0.000001)
)
```

Podemos visualizarlo:

```{r fig.height=12, fig.width=20}
rpart.plot(cart_final$finalModel)
```


Y generamos las predicciones finales:

```{r}
y_preds <- predict(cart_final, test)
```

Generamos nuestra matriz de confusión:

```{r}
confusionMatrix(y_preds, test$imp_inglab1)
```

¿Qué se puede decir del árbol de decisión? ¿Cómo funciona?


## Random Forest

La idea ahora es poder entrenar un modelo random forest. Pero... lo van a hacer ustedes. Para ello, deberán consultar [la documentación de caret](http://topepo.github.io/caret/available-models.html) y buscar cuál es la mejor función para entrenar el modelo y sus hiperparámetros.

Pista: buscar el método `ranger`

Usaremos la partición original entre Train-Test. Recordemos los pasos a seguir

1. Generar el esquema de validación cruzada
2. Generar el grid de hiperparámetros
3. Tunear el modelo
4. Seleccionar y estimar el modelo final
5. Validar y evaluar contra test-set


```{r}
set.seed(5699)
cv_index_rf <- createFolds(y=train$imp_inglab1,
                        k=5,
                        list=TRUE,
                        returnTrain=TRUE)

fitControlrf <- trainControl(
        index=cv_index_rf,
        method="cv",
        number=5,
        summaryFunction = twoClassSummary,
        classProbs=TRUE
        )

# Generar grid

grid_rf <- expand.grid(mtry=c(5,10,15, 25),
                       min.node.size=c(5,10,15,20),
                       splitrule='gini'
        )

```


```{r eval=FALSE, include=TRUE}
# Tunear
t0 <- proc.time()
rf_fit_orig <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "ranger", 
                 trControl = fitControlrf,
                 tuneGrid = grid_rf
)
proc.time() -  t0

#saveRDS(rf_fit_orig, '../models/rf_fit_orig.RDS')
```

```{r include=FALSE}
rf_fit_orig <- readRDS('../models/rf_fit_orig.RDS')
```

```{r}
rf_fit_orig

y_preds_orig <- predict(rf_fit_orig, test)
```

- ¿Qué pueden decir de este modelo? 
- ¿Funciona mejor que un árbol individual? 
- ¿Qué sucede con su performance entre los `miss`


## Datasets desbalanceados... ¿qué hacer?

Efectivamente, parte del problema de este dataset es que tenemos una variable dependiente sumamente desbalanceada:

```{r}
ggplot(train) + 
        geom_bar(aes(x=imp_inglab1))
```

#### Matriz de confusión

Ya estamos familiarizades con la matriz de confusión. Básicamente, es una forma rápida de comparar las predicciones del modelo con los valores reales. 

```{r}
prop.table(table(y_preds_orig, test$imp_inglab1))
```

Así, la métrica más simple es la siguiente:

$Accuracy = \frac{TP + TN}{N} = \frac{TP + TN}{TP + TN + FP + FN}$

Simplemente, calcula la proporción de casos bien clasificados ($TP + TN$) sobre el total. Pero esta métrica tiene algunos problemas. El más importante es que tiende a ser una medida engañosa de la performance de un modelo en un dataset desbalanceado.

Un primer punto importante es poder seleccionar una métrica adecuada para evaluar nuestro modelo. Existen dos grandes familias de medidas:

- Dependiente del umbral: Esto incluye métricas como precisión, recall y puntaje F1, que requieren una matriz de confusión para ser calculada usando un límite duro en las probabilidades pronosticadas. Estas métricas suelen ser bastante pobres en el caso de clases desequilibradas, ya que el software estadístico utiliza de manera inapropiada un umbral predeterminado de 0,50, lo que hace que el modelo prediga que todas las observaciones pertenecen a la clase mayoritaria.

$Precision = \frac{TP}{TP+FP}$ Ratio entre verdaderos positivos y los que el modelo  clasifica como positivos. ¿Qué proporción de los que clasifcamos como positivos son efectivamente positivos?

$Recall =  \frac{TP}{TP+FN}$ También llamada, $Sensitividad$, Ratio entre los positivos bien clasificados y el total de los que son positivos en realidad. ¿Qué proporción de los que efectivamente positivos hemos clasificado bien?

$F1-Score = 2 \times \frac{Precision \times Recall}{Recall + Precision}$

- Independiente del umbral: Esto incluye métricas como área bajo la curva ROC (AUC), que cuantifica la tasa positiva verdadera en función de la tasa de falsos positivos para una variedad de umbrales de clasificación. Otra forma de interpretar esta métrica es la probabilidad de que una instancia positiva aleatoria tenga una probabilidad estimada más alta que una instancia negativa aleatoria.


#### ROC Curve o Área Under the Curve (AUC)




En líneas generales, existen cuatro formas generales de lidiar con el problema de las clases desbalanceadas:

- Usar modelos ponderados
- _Down-sampling_ es decir, submuestrear las instancias sobrerrepresentadas
- _Up-sampling_, es decir, sobremuestrar las instancias subrrepresentadas
- _Synthetic minority sampling technique (SMOTE)_: hace _down-sampling_ de la clase mayoritaria y "sintetiza" nuevas instancias minoritarias mediante interpolación


### Utilizar pesos en el modelo

Para esto podemos utilizar el argumento `weights` en la función `train` -esto supone que el modelo seleccionado puede manejar pesos-...


```{r}
model_weights <- ifelse(train$imp_inglab1 == "non_miss",
                        (1/table(train$imp_inglab1)[1]) * 0.5,
                        (1/table(train$imp_inglab1)[2]) * 0.5)

fitControlrf$seeds <- rf_fit_orig$control$seeds
```


Y entrenamos el modelo:

```{r}
t0 <- proc.time()
rf_fit_wei <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "ranger", 
                 trControl = fitControlrf,
                 tuneGrid = grid_rf,
                 weights = model_weights,
                 metric='ROC'
)
proc.time() -  t0

#saveRDS(rf_fit_wei, '../models/rf_fit_wei.RDS')

```

```{r include=FALSE}
rf_fit_wei <- readRDS('../models/rf_fit_wei.RDS')
```


### Métodos de remuestreo

Probemos, ahora, métodos de remuestreo: _up sampling_, _down sampling_ y _SMOTE_.

```{r}

# Nuevo fitControl con upsample

fitControlrf_imb <- trainControl(
        index=cv_index_rf,
        method="cv",
        number=5,
        summaryFunction = twoClassSummary,
        classProbs=TRUE,
        sampling = 'up'
        )

```

```{r eval=FALSE, include=TRUE}

# Tunear upsample

fitControlrf_imb$seeds <- rf_fit_orig$control$seeds

t0 <- proc.time()
rf_fit_up <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "ranger", 
                 trControl = fitControlrf_imb,
                 tuneGrid = grid_rf,
                 metric='ROC'
)
proc.time() -  t0

saveRDS(rf_fit_up, '../models/rf_fit_up.RDS')
```

```{r include=FALSE}
rf_fit_up <- readRDS('../models/rf_fit_up')

```



```{r eval=FALSE, include=TRUE}
# Tunear downsample

fitControlrf_imb$sampling <- "down"
fitControlrf_imb$seeds <- rf_fit_orig$control$seeds


t0 <- proc.time()
rf_fit_down <-  train(imp_inglab1 ~ . , 
                 data = train, 
                 method = "ranger", 
                 trControl = fitControlrf_imb,
                 tuneGrid = grid_rf,
                 metric='ROC'
)
proc.time() -  t0

#saveRDS(rf_fit_down, '../models/rf_fit_down.RDS')

```

```{r include=FALSE}
rf_fit_down <- readRDS('../models/rf_fit_down.RDS')
```


```{r eval=FALSE, include=FALSE}
rf_fit_smote <- readRDS('../models/rf_fit_smote.RDS')
```


Colocamos todos los modelos en una lista:

```{r}
model_list <- list(original = rf_fit_orig,
                   weighted = rf_fit_wei,
                   down = rf_fit_down,
                   up = rf_fit_up)

```


Extraigamos algunas métricas resumen de las matrices de confusión:

```{r}
extract_conf_metrics <- function(model, data, obs){
        preds <- predict(model, data)        
        c<-confusionMatrix(preds, obs)
        results <- c(c$overall[1], c$byClass)
        return(results)
}

```


```{r}

model_metrics <- model_list %>%
        map(extract_conf_metrics, data=test, obs = test$imp_inglab1) %>%
        do.call(rbind,.)

```

```{r}
model_metrics
```




