---
title: "Clase 1 - Árboles de decisión"
author: "Diplomatura en Ciencias Sociales Computacionales"
date: '2023-03-03'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Árboles de decisión

## El problema

Segumos con nuestro problema central: predecir los ingresos de la ocupación principal (`p21`) en la EPH del segundo trimestre del 2015. Pero en esta oportunidad queremos evaluar si podemos predecir la no respuesta. Es decir, entrenar un modelo que nos permita predecir qué tan probable es que una persona NO responda ingresos.

Ya hemos preprocesado los datos y estamos listos

Lo primero que tenemos que hacer es importar las librerías con las que vamos a trabajar:

```{r message=FALSE, warning=FALSE}
library(tidymodels)
```

```{r}
load('../2022/clase_2/data/EPH_2015_II.RData')

data$pp03i<-factor(data$pp03i, labels=c('1-SI', '2-No', '9-NS'))

data$intensi<-factor(data$intensi, labels=c('1-Sub_dem', '2-SO_no_dem', 
                                            '3-Ocup.pleno', '4-Sobreoc',
                                            '5-No trabajo', '9-NS'))

data$pp07a<-factor(data$pp07a, labels=c('0-NC',
                                        '1-Menos de un mes',
                                        '2-1 a 3 meses',
                                        '3-3 a 6 meses',
                                        '4-6 a 12 meses',
                                        '5-12 a 60 meses',
                                        '6-Más de 60 meses',
                                        '9-NS'))


data <- data %>%
        mutate(imp_inglab1=factor(imp_inglab1, labels=c('non_miss','miss')))

summary(data$imp_inglab1)

```

Ahora, nuestra variable a predecir es el indicador `imp_inglab`. Por ende, eliminamos la $p21$.

```{r}
data <- data %>%
        select(-p21)
```

Lo primero que vamos a hacer es crear una partición de datos:

```{r}
set.seed(123)
eph_split <- initial_split(data)
eph_train <- training(eph_split)
eph_test <- testing(eph_split)
```

Y hacemos las muestras cross-validation:

```{r}
set.seed(234)
eph_cv <- vfold_cv(eph_train)
eph_cv
```

Ahora, armarmos la estructura del modelo pero con parámetros que queremos tunear. Por eso ponemos la función `tune()` en la definición de los distintos parámetros: el umbral de la métrica de pureza para definir la complejidad del árbol, la profundidad del árbol y el mínimo de variables que tiene que tener cada partición del nodo.

```{r}
tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_spec
```

Con la función `grid_regular()` armamos una serie de combinaciones aleatorias posibles para los parámetros del modelo. Le decimos que nos de 4 niveles de valores posibles para cada uno de los parámetros, y que los combine.

```{r}
tree_grid <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 4)

tree_grid
```

Ahora vamos a probar los posibles valores que puede adoptar el modelo con los distintos parámetros en las muestras con cross-validation.

```{r}
doParallel::registerDoParallel()

set.seed(345)
tree_rs <- tune_grid(
  tree_spec,
  imp_inglab1 ~ .,
  resamples = eph_cv,
  grid = tree_grid,
  metrics = metric_set(accuracy, precision,
                       recall, f_meas)
)

tree_rs

```

Podemos ver cómo fue la performance del modelo con cada uno de los parámetros:

```{r}
collect_metrics(tree_rs)
```

Y también podemos graficarlo de manera sencilla con la función `autoplot`.

```{r}
autoplot(tree_rs) 
```

Parecería que este modelo funciona mejor cuando no tiene tanta profundidad. Podemos ver, y seleccionar, el modelo que mejor funciona de acuerdo a la métrica de evaluación que elijamos usando las funciones `show_best()` y `select_best()`.

```{r}
show_best(tree_rs, "f_meas")
select_best(tree_rs, "f_meas")
```

Estas funciones de arriba nos devolvían un dataset con la evaluación de los modelos según distintos parámetros. Pero a nosotres nos interesa tomar los mejores hiperparámetros del grid y pasarlos como modelo final a tidymodels. Esto lo vamos a hacer con la función `finalize_model()`.

```{r}
best_model <- select_best(tree_rs, "f_meas")
final_tree <- finalize_model(tree_spec, best_model)

final_tree

```

Hasta acá seleccionamos el mejor modelo del grid de parámetros para nuestro árbol de decisión con la base que vamos a trabajar. Sin embargo, todavía no está *fiteado*. Hacer el fit de un modelo quiere decir que lo hacemos que aprenda de un set de datos específico, con determinadas variables y cierto output. Es decir, lo hacemos "encajar". En este caso, lo queremos fitear a nuestro set de entrenamiento. Esto lo vamos a hacer con la función `fit()`.

```{r}
final_fit <- fit(final_tree, imp_inglab1 ~ ., eph_train)

final_fit
```

Imprimir el modelo fiteado nos devuelve un bloque de texto que nos muestra cómo está construido el árbol. Al estar trabajando con tantas variables, resulta bastante difícil de leer. Podemos ver cuáles son las variables más importantes en este árbol de decisión para predecir si el caso fue imputado o no utilizando el paquete adicional `vip`, que calcula el Variable Importance score de los features de input.

```{r}
library(vip)

final_fit %>%
  vip(geom = "col", aesthetics = list(fill = "dodgerblue3", alpha = 0.7)) 

```

Acá podemos ver que las variables más importantes son el aglomerado y la región. También podemos graficar el árbol en un dendograma usando la función `rpart.plot`.

```{r}
library(rpart.plot)
rpart.plot(final_fit$fit, type = 0)

```

Hasta acá, tenemos información sobre cómo funciona nuestro modelo y su performance en el set de entrenamiento. Sin embargo, lo que nos interesa es ver cómo funciona sobre un grupo de observaciones en las cuales no fue entrenado. Queremos hacer la prueba de cómo funcionaría con datos "reales", y por eso tenemos que evaluar las métricas del modelo en el test set.

Para eso, vamos a usar la función `predict()` sobre `eph_set`, lo agregamos como columna y vamos a evaluar sus métricas.

```{r}
eph_test <- final_fit %>%
  predict(eph_test) %>%
  bind_cols(eph_test, .)

eph_test
```

Para evaluar el modelo usamos distintas funciones del paquete `yardstick` que está en tidymodels. \

```{r}
class_metrics <- metric_set(accuracy, f_meas, precision, recall)
class_metrics(eph_test, truth = imp_inglab1, estimate = .pred_class)

```
