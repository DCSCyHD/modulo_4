---
title: "Clase 4 - Boosting"
author: 
  - Diplomatura en Ciencias Sociales Computacionales Digitales (IDAES-UNSAM)
  - Martín Schuster y Laia Domenech Burin
date: '2023'
output: 
  rmdformats::readthedown
---

```{r}
# Instalar engine xgboost
#install.packages("xgboost")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(themis)
library(rules)
```

# Cierre de ensambles

Vamos a terminar de ver ensambles con modelos de boosting. Como vimos en la parte teórica, estos modelos tienen la característica de realizar un aprendizaje iterativo sobre estimadores que se construyen secuencialmente. Vamos a trabajar sobre la base de la ENUT para predecir si se realiza TD o no, y comparar con los resultados de los modelos anteriores.

Primero, hacemos el preprocesamiento:

```{r}
library(tidyverse)

data <- read_delim("./data/enut2021_base.txt", delim = "|")

data <- data %>% select(ID, SEXO_SEL, EDAD_SEL, TCS_GRUPO_DOMESTICO, CONDICION_ACTIVIDAD_AGRUPADA,   
                        NIVEL_EDUCATIVO_AGRUPADO, CANT_DEMANDANTES_TOTAL, CANT_NODEMANDANTES_TOTAL,
                        BHCH04_SEL, BHDC_SEL) %>% 
  mutate(realiza_domest = as.factor(case_when(
    TCS_GRUPO_DOMESTICO > 60 ~ "Realiza",
    TRUE ~ "No realiza")))
 
data <- data %>% mutate_at(
                   vars(SEXO_SEL), 
                    ~as.factor(case_when(
                      . == 1 ~ "Mujer",
                      . == 2 ~ "Varon"
                    )))
 

data <- data %>% mutate_at(vars(CONDICION_ACTIVIDAD_AGRUPADA), 
                   ~as.factor(case_when(
                     . == 1 ~ "Ocupado",
                     . == 2 ~ "No ocupado"
                   )))
 
data <- data %>% mutate_at(vars(BHCH04_SEL), 
                   ~as.factor(case_when(
                     . == 1 ~ "Jefe/a",
                     . == 2 ~ "Conyuge/pareja",
                     . == 3 ~ "Hijo/a",
                     . == 4 ~ "Hijastro/a",
                     . == 5 ~ "Yerno/nuera",
                     . == 6 ~ "Nieto/a",
                     . == 7 ~ "Padre o madre",
                     . == 8 ~ "Suegro/a",
                     . == 9 ~ "Hermano/a",
                     . == 10 ~ "Cuniado/a",
                     . == 11 ~ "Sobrino/a",
                     . == 12 ~ "Abuelo/a",
                     . == 13 ~ "Otro familiar",
                     . == 14 ~ "Otro no familiar")))


 
 data <- data %>% mutate_at(vars(BHDC_SEL), 
                   ~as.factor(case_when(
                     . == 0 ~ "No es demandante de cuidado",
                     . == 1 ~ "Es demandante de cuidado"
                   )))

data<- data %>% select(-TCS_GRUPO_DOMESTICO)

```

## Árboles simples y modelos de bagging

Recordemos los modelos previos.

```{r}
decision_tree <- readRDS('./data/validation_test_dt.RDS')
bagging <- readRDS('./data/validation_test_bag.RDS')
random_forest <- readRDS('./data/validation_rf.RDS')

datasets <- list(decision_tree, bagging, random_forest)

class_metrics <- metric_set(precision, recall,
                       accuracy, f_meas)

metrics_dt <- roc_auc(decision_tree, truth = realiza_domest, ".pred_No realiza") %>% 
  bind_rows(., class_metrics(decision_tree, truth = realiza_domest, estimate = .pred_class)) %>% 
  mutate(modelo = "decision_tree")

metrics_bagging <- roc_auc(bagging, truth = realiza_domest, ".pred_No realiza") %>% 
  bind_rows(., class_metrics(bagging, truth = realiza_domest, estimate = .pred_class))%>% 
  mutate(modelo = "bagged_tree")

metrics_rf <- roc_auc(random_forest, truth = realiza_domest, ".pred_No realiza") %>% 
  bind_rows(., class_metrics(bagging, truth = realiza_domest, estimate = .pred_class))%>% 
  mutate(modelo = "random_forest")

rbind(metrics_dt, metrics_bagging, metrics_rf) %>%
ggplot(aes(x = .metric, y = .estimate, fill = modelo))+
  geom_col(position = "dodge")+
  scale_fill_viridis_d()+
  theme_minimal()

```

-   ¿Qué modelo funciona mejor? ¿Por qué?

## Boosting

Vamos a realizar un modelo de boosting con `tidymodels`. Primero, importamos la librería:

```{r, eval = F}
library(tidymodels)
```

Hacemos la partición de datos y preprocesamiento del workflow:

```{r}
set.seed(123)

split <- initial_split(data)
train <- training(split)
test <- testing(split)

recipe <- recipe(realiza_domest ~ ., data = train)%>%
  update_role(ID, new_role = "id") %>%
  step_other(BHCH04_SEL, threshold = 0.2)%>%
  # El engine XGBoost necesita que transformemos las variables categoricas a dummy
  step_dummy(SEXO_SEL) %>%
  step_dummy(CONDICION_ACTIVIDAD_AGRUPADA) %>%
  step_dummy(BHCH04_SEL) %>%
  step_dummy(BHDC_SEL) %>%
  step_downsample(realiza_domest, under_ratio = 1)

wf <- workflow() %>%
  add_recipe(recipe)
```

Ahora, viene la parte de especificar la información del modelo. Como estamos trabajando con boosting, empezamos pasando el parámetro `boost_tree()`. Para elegir entre los tipos de modelos de boosting posibles, hay que modificar el parámetro de `set_engine()`. Por default, tidymodels usa el parámetro `xgboost`para modelos de boosting. Existen otros engines, como el C5.0 que sigue un procedimiento similar a AdaBoosting, un ensamble de árboles que hacen un voto ponderado para asignar a la clase final.

```{r}
bt_spec <- boost_tree(
  # Estos son los hiperparámetros mas utilizados en ensamble,
  # learn_rate es propio gradiant boosting
  trees = tune(), 
  mtry = tune(),
  tree_depth = tune(),
  learn_rate = tune()) %>%
  # Usamos el engine xgboost para poder utilizar el hiperparámetro learn_rate
  set_engine("xgboost") %>%
  set_mode("classification")

bt_spec %>% translate()
```

Ahora, vamos a hacer las muestras de cross-validation para tunear los hiperparámetros. Haremos un grid de 10 cruces para distintas combinaciones de la cantidad de árboles y la mínima cantidad de particiones.

```{r}
doParallel::registerDoParallel()

tune_wf <- wf %>%
  add_model(bt_spec)
 
set.seed(912)

folds <- vfold_cv(train)
 
tune_params <- tune_wf %>%
   tune_grid(folds,
             metrics = metric_set(precision, recall,
                               roc_auc, f_meas),
             grid = 20)
```

```{r}
autoplot(tune_params)
```

```{r}
tune_params %>%
  collect_metrics()
```

Ahora, seleccionamos el mejor modelo en base a f1 y lo finalizamos.

```{r}
best_model <- select_best(tune_params, metric = "roc_auc")

final_model <- finalize_model(bt_spec, best_model)
```

```{r}
final_model
```

Para finalizar, actualizamos el modelo tuneado en el workflow y lo fiteamos.

```{r}
tree_boost <- wf %>%
   update_model(final_model)

fit_tree <- tree_boost %>% fit(train)
```

Guardamos el modelo para usarlo a futuro.

```{r echo = FALSE}
write_rds(fit_tree, "./model/boosting_fit.RDS")
```

Ahora, vamos a predecir sobre el test set, obtener las métricas de evaluación y compararlas con los otros modelos.

```{r}
boost_valid <- fit_tree %>%
  predict(test) %>%
  bind_cols(., test)

boost_valid <- predict(fit_tree, test, type = "prob") %>%
  bind_cols(boost_valid, .)


class_metrics <- metric_set(precision, recall,
                       accuracy, f_meas)

boost_metrics <- roc_auc(boost_valid, truth = realiza_domest, ".pred_No realiza") %>%
  bind_rows(class_metrics(boost_valid, truth = realiza_domest, estimate = .pred_class)) %>%
  mutate(modelo = "Boosting")

rbind(metrics_dt, metrics_bagging, metrics_rf, boost_metrics) %>%
ggplot(aes(x = .metric, y = .estimate, fill = modelo))+
  geom_col(position = "dodge")+
  scale_fill_viridis_d()+
  theme_minimal()

```

```{r}
boost_metrics
```

Teniendo en cuenta qué significa cada métrica de evaluación, ¿qué pueden decir de los 4 modelos que probaron?

-   ¿Cuál captura mayor cantidad de casos positivos?

-   ¿Cuál captura con mayor exactitud los casos positivos?

-   ¿Cuál funciona mejor en base a esos dos criterios? ¿Y peor?

```{r}
# Para feature imporance usando el engine xgboost tengo que importar la libreria
# y luego acceder al modelo entrenado con $fit$fit$fit, esto es muy propio de xgboost
library(xgboost)
xgb.importance(model=fit_tree$fit$fit$fit)
```
